---
title: 'Convolutional Neural Networks From Scratch Part 3 - Convolution'
description: 'Convolution and a Convolutional Layer'
pubDate: 'Sep 24 2024'
heroImage: '/neural net hero.jpg'
---
import Latex from '../../components/Latex.astro';
import LatexBlock from '../../components/LatexBlock.astro';
import Guesser from '../../components/Guesser.astro';
import Note from '../../components/Note.astro';
import forward from '../../videos/forward.webm';

<h3>Convolutional Layers</h3>

The only difference between a feedforward neural net and a convolutional neural net is the presence of **convolutional layers**. Before we dig into how we implement a convolutional layer, let's dig into the mathematics behind convolution and gain some intuition for why convolutional layers are so powerful for image processing tasks.

<h5>What is Convolution?</h5>

Let's pretend we are tracking the pressure readings on a submarine at certain times.

We can get the pressure reading at a certain time with the function:

<LatexBlock formula='x(t)' />

Now suppose our pressure sensor is noisy. We would like to take an average over several measurements to get a less noisy estimate of the submarine's pressure. We'd like the most recent measurements to be weighted more heavily. We can do this by taking a weighted average that gives more weight to recent pressure readings. 

We can construct a new function <Latex formula='w(a)' /> where <Latex formula='a' /> is the age of the reading. If we apply this weighted average at every moment <Latex formula='t' /> we end up with a new function:

<LatexBlock formula='s(t) = \int_{-\infty}^\infty x(a)w(t - a)da' />

This is called a **convolution**. It's essentially a combination of two functions by sliding one function (in this case <Latex formula='w(a)' />) over another (<Latex formula='x(t)' />), multiplying them together and and integrating the result.

<h5>Discretized Convolution</h5>

It's not realistic for our pressure sensor to make a reading at every possible timestep <Latex formula='t' />. Instead, we're probably going to be taking readings at **discrete** time steps. For example, we might take a reading every 10 seconds.

This is similar to the continuous convolution above, only the integral becomes a sum:

<LatexBlock formula='s(t) = \sum_{a = -\infty}^\infty x(a)w(t - a)' />

In convolution terminology:
- <Latex formula='x(t)' /> is the **input**.
- <Latex formula='w(a)' /> is the **kernel**. It's what's swept across the input.
- <Latex formula='s(t)' /> is the **feature map**. It's the result of the convolution.

<Note>You may notice the <Latex formula='\infty' /> in the summation and wonder how it's possible to perform a convolution from negative infinity to infinity. In practice, we implement this by implicitly padding the input with 0s everywhere that is outside the finite range of the input.</Note>
<br/>

<h5>Multidimensional Discretized Convolution</h5>

In the above example we were doing convolution over a single dimension (time). We will be dealing with image data, though and that is 2-dimensional.

To perform discretized convolution in 2 dimensions we need to use a 2-dimensional kernel:

<LatexBlock formula='S(i, j) = \sum_m\sum_nI(m, n)K(i - m, j - n)' />

Where the kernel is <Latex formula='mxn' /> (m is the height, n is the width).

Convolution is *commutative*, so we can equivalently write:

<LatexBlock formula='S(i, j) = \sum_m\sum_nI(i - m, j - n)K(m,n)' />

<Note>We arrive at this commutation by **rotating** the kernel 180 degrees. We also need to rotate the kernel 180 degrees again during backpropagation. If we do not rotate the kernel, what we're doing is not technically convolution, and is instead **cross-correlation**. Practically speaking, rotating the kernel doesn't matter for the purposes of machine learning so we won't be doing it in our implementation for the sake of simplicity. This means that our equation becomes: <LatexBlock formula='S(i, j) = \sum_m\sum_nI(i + m, j + n)K(m,n)' /></Note>

To recap there are a few things we need to perform convolution on an image:

1. An input image:

<img src="/input.png" width="300" alt="input" />


2. A kernel: 

<img src="/kernel.png" width="300" alt="kernel" />

And lastly, we need to pick a **stride**. A stride is how many pixels you move the kernel with each iteration. For the simplicity of this demonstration we'll be picking a stride of 2 in both dimensions, but the stride could be different across different dimensions. 

As discussed, we slide the kernel across the input image starting from the top left and continue on until we reach the end of the image. At each iteration, we multiply all of the pixel values inside the kernel window by the **kernel weights** and then add those products together to get one final value, <Latex formula='z_i' /> which will be our pixel value.

<img src="/conv1.png" width="800" alt="kernel" />

We continue this process until we've swept the kernel acrosss the entire input image. The final output is our **feature map**.

See the video below to view a visual of how we convolve a kernel over a 2D image:

<video width="100%" controls>
 <source src={forward} type="video/webm" />
</video>

<br /><br />

<h5>Why Convolution?</h5>

You may be asking yourself at this point what exactly this acheives, and why it is beneficial for character recognition. 

Below is an example of an image of a cat. I've specifically designed a simple kernel which will be convolved with the image to produce a feature map. Click the button to see what the feature map looks like. 

<Convolution />

This is essentially a simple **vertical edge detector**. Detecting vertical edges is useful for many object recognition tasks, including reading handwriting. For example, the absence of any vertical edges suggests that a digit might be a 0 or and 8 or something.

There are many other kinds of useful kernels possible.

For example, use the spin buttons on the kernel input to adjust the values to:

<LatexBlock formula='\begin{bmatrix} 0 & -1 & 0 \\ -1 & 5 & -1 \\  0 & -1 & 0 \end{bmatrix}' />

This kind of kernel is a **sharpening kernel** and it can increase the contrast around the edges making the image appear sharper. 

Feel free to play around with different kernels and see their result on the input image. 

It should be starting to make sense to you how convolving these kernels around the image could help extract features that would be useful for image processing. However, picking the right kernels for the task might be difficult. 

This is the beauty of convolutional neural nets. We don't have to make any decisions about what the kernels will be or what features are important. At no point will we be hand-designing a kernel to be swept over any of the training data. The training of the neural net will learn that for us -- and it will (hopefully) learn the optimal ones for our task. 

<h5>A Simple Implementation</h5>

Before jumping into the implementation of the convolutional layer class, let's first start by implementing a simple 2D convolution.

First we need some input data. We'll just generate fake data for now:

```python
import numpy as np

# input initialization
image_height = image_width = 6
image = np.random.randn(image_height, image_width)
```

And now a kernel. Our kernel will be 2x2:

```python
# kernel initialization
kernel_height = kernel_width = 2
kernel = np.random.randn(kernel_height, kernel_width)
```

And finally we need a stride:

```python
# stride initialization
stride = 1 
```

Now, we need to prepare the matrix/tensor that will be the output of our convoltuion. We'll initialize it to be all zeros and fill it in element by element as we perform the convolution. 

The size of the output can be calculated with this equation:

<LatexBlock formula='\text{Output Size} = \dfrac{\text{image size} - \text{kernel size}}{\text{stride}} + 1' />

With a stride of 1 this becomes:

<LatexBlock formula='\text{Output Size} = \text{image size} - \text{kernel size}+ 1' />

If we slide the kernel across the width of the image, the last place we can place the kernel window is at <Latex formula='\text{image width} - \text{kernel width}' />. In our case that would be in the 4th column. We add 1 always because we need to account for the starting position.

```python
# output initialization
out_height = image_height - kernel_height // stride + 1
out_width = image_width - kernel_width // stride + 1

output_size = (out_height, out_width)

output = np.zeros(output_size)
```

Now that our output feature map is initialized we can begin sliding the kernel across the image. At each iteration we multiply the kernel's weights with the pixel it is overlapping, then sum them all up to get the pixel value for the feature map at that position:

```python
# convolutional loop
for row in range(out_height):
    for col in range(out_width):
        h_start = row * stride
        h_end = h_start + kernel_height
        w_start = col * stride
        w_end = w_start + kernel_width
        
        window = image[h_start:h_end,w_start:w_end]
        output[row, col] = np.sum(kernel * window)
```

<h5>Multiple Output Channels</h5>

So far, this is a simple implementation of convolution. In this little toy scenario it works, but there's more to say on the matter. In a real convolutional layer we often want to produce more than one feature map during the forward pass. You can think of this as having multiple different kernels in that layer that are meant to extract different features from the image. 

The naive implementation of this would be to have some kind of array of kernels in the convolutional layer and loop over them, convolving each kernel with the image. This would be incredibly slow. Instead, we will add an extra dimension to our kernel tensor that corresponds to the number of **output channels**:

```python
# kernel initialization
output_channels = 16
kernel_height = kernel_width = 2
kernel = np.random.randn(output_channels, kernel_height, kernel_width)
```

Each output channel will represent one kernel and produce one feature map. To do the convolution in one vectorized operation we need to take advantage of numpy's **broadcast semantics**. We can effectively copy the input image 16 times into a tensor whose dimensions are appropriate to perform the convolution, and then produce all 16 feature maps at once. 

Here's how it works:
1. You compute the "window" the kernel will be applied to.
2. That window will have the shape `(kernel_width, kernel_height)`.
3. The kernel has the shape `(output_channels, kernel_width, kernel_height)`.

This means that based on numpy's broadcasting rules, the first thing that will happen is that the arrays will be reshaped to be compatible with broadcasting:

```python
kernel_shape = (output_channels, kernel_width, kernel_height)
window_shape = (1, kernel_width, kernel_height)
```

Now, when you multiply the window values by the kernel values, it will copy whatever is in the first dimension of the window to all of the output channels. This allows you to generate all of the output feature maps at once.

We also need to modify the output to be the correct shape:

```python
# output initialization
out_height = image_height - kernel_height // stride + 1
out_width = image_width - kernel_width // stride + 1

output_size = (output_channels, out_height, out_width)

output = np.zeros(output_size)
```

And our convolution loop to accomodate the output channels:

```python
# convolutional loop
for row in range(out_height):
    for col in range(out_width):
        h_start = row * stride
        h_end = h_start + kernel_height
        w_start = col * stride
        w_end = w_start + kernel_width
        
        window = image[h_start:h_end,w_start:w_end]
        output[:, row, col] = np.sum(kernel * window, axis=(1, 2))
```

Note that this time we need to specify which axes we wish to sum over. The resukt of `kernel * window` will have the shape: `(output_channels, kernel_height, kernel_width)`. Each output channel is really its own independent kernel. We don't want to sum over all of the kernels at once. This is why we specify to numpy that we're summing over `axis=(1, 2)` only.

<h5>Multiple Input Channels</h5>

There's another thing to consider: multiple input channels. It's not a complicated change. We just need to add an extra dimension to the input corresponding to the number of input channels, and the same dimension to the kernel tensor:

```python
# input initialization
input_channels = 3
image_height = image_width = 6
image = np.random.randn(input_channels, image_height, image_width)
```

```python
# kernel initialization
output_channels = 16
kernel_height = kernel_width = 2
kernel = np.random.randn(output_channels, input_channels, kernel_height, kernel_width)
```

```python
# convolutional loop
for row in range(out_height):
    for col in range(out_width):
        h_start = row * stride
        h_end = h_start + kernel_height
        w_start = col * stride
        w_end = w_start + kernel_width
        
        window = image[:,h_start:h_end,w_start:w_end]
        output[:, row, col] = np.sum(kernel * window, axis=(1, 2, 3))

```

<h5>Batch Processing</h5>

There's one final consideration before we can consider on our implemenation of convolution complete: batches. Like our other layers, we would like our convolutional layer to process an entire batch at once. We could simply loop through each input in the batch and do exactly what we did above. However, we can once again use numpy's broadcast semantics to our advantage to process the entire batch at once.

First, we'll change our input to be in batches:

```python
# input initialization
batch_size = 32
input_channels = 3
image_height = image_width = 6
image = np.random.randn(batch_size, input_channels, image_height, image_width)
```

The kernel stays the same.

Adding this extra dimension to the input though has messed up our broadcasting. Because our out input tensor now has the batch dimension, the broadcasting of the input across all of the output channels during convolution won't happen. To fix this, all we need to do is arbitrarily add an extra dimension after the batch dimension during the convolution loop so that our input is broadcast again:

```python
# output initialization
out_height = image_height - kernel_height // stride + 1
out_width = image_width - kernel_width // stride + 1

output_size = (batch_size, output_channels, out_height, out_width)

output = np.zeros(output_size)
```

Now, during the convolution loop we will add an extra dimension to the "window". This extra dimension will go inbetween the the `batch_size` and the `input_channels` dimension to facilitate the broadcasting with numpy's `expand_dims()` function: 

```python
# convolutional loop
for row in range(out_height):
    for col in range(out_width):
        h_start = row * stride
        h_end = h_start + kernel_height
        w_start = col * stride
        w_end = w_start + kernel_width
                
        window = image[:,:,h_start:h_end,w_start:w_end]
        print(np.expand_dims(window, 1).shape)
        print(kernel.shape)
        output[:,:, row, col] = np.sum(np.expand_dims(window, 1) * kernel, axis=(2, 3, 4))
```

Here is the full convolution code:

```python
import numpy as np

# input initialization
batch_size = 32
input_channels = 3
image_height = image_width = 6
image = np.random.randn(batch_size, input_channels, image_height, image_width)

stride = 1 

# kernel initialization
output_channels = 16
kernel_height = kernel_width = 2
kernel = np.random.randn(output_channels, input_channels, kernel_height, kernel_width)

# output initialization
out_height = image_height - kernel_height // stride + 1
out_width = image_width - kernel_width // stride + 1

output_size = (batch_size, output_channels, out_height, out_width)

output = np.zeros(output_size)

# convolutional loop
for row in range(out_height):
    for col in range(out_width):
        h_start = row * stride
        h_end = h_start + kernel_height
        w_start = col * stride
        w_end = w_start + kernel_width
                
        window = image[:,:,h_start:h_end,w_start:w_end]
        output[:,:, row, col] = np.sum(np.expand_dims(window, 1) * kernel, axis=(2, 3, 4))

```

<Note>If you find all of this fiddling with the tensor dimension confusing: **don't worry about it**. These are really implementation details that are optimizations more than anything else. The most important thing to understand is what 2D convolution actually is.</Note>

<h5>The Conv2D Class</h5>

With a reasonably robust convolution implementation we can build out a convolutional layer class.

```python
class Conv2D:
    def __init__(self, in_channels, out_channels, kernel_size, stride=1):
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.1
        self.stride = stride

    def forward(self, batch):
        batch_size, in_channels, input_height, input_width = batch.shape
        out_channels, _, kernel_height, kernel_width = self.kernel.shape

        out_height = (input_height - kernel_height) // self.stride + 1
        out_width = (input_width - kernel_width) // self.stride + 1
        
        output = np.zeros((batch_size, out_channels, out_height, out_width))
        for row in range(out_height):
            for col in range(out_width):
                h_start = row * self.stride
                h_end = h_start + kernel_height
                w_start = col * self.stride
                w_end = w_start + kernel_width
                
                window = batch[:,:,h_start:h_end,w_start:w_end]
                output[:,:, row, col] = np.sum(np.expand_dims(window, 1) * self.kernel, axis=(2, 3, 4))
                    
        return output
```

This is essentially the same as our implementation in the previous section reified into a layer class. 

<h3>Max Pooling</h3>

In a typical convolutional neural net, the convolution happens in three stages:
1. The 2D convolution proper, which we covered last section.
2. That output is passed through a nonlinearity like ReLU. We've covered how those layers work in previous sections.
3. The output of the nonlinearity goes through a **pooling** layer. 

Pooling layers replace the output of the previous layer with a kind of summary of that output. They can introduce **translation invariance** i.e., the network becomes insensitive to small position changes in the input. For example, if we train our network on an image of a handwritten "5", we don't want our network to be confused if we then show it the same "5" shifted a few pixels to the right. This kind of translation invariance can help reduce overfitting and greatly increase the performance of the network overall. 

There are different kinds of pooling operations, but we will focus on the most common and simplest: **max pooling**.

Similar to convolution, max pooling sweeps a kernel across the input (a feature map). Unlike convolution, it simply takes the maximum value in the kernel and puts it inside the output.

